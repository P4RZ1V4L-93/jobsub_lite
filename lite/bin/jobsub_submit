#!/usr/bin/env python

import os
import os.path
import sys
import glob
import argparse
import jinja2 as jinja


def parse_dagnabbit(parser, srcdir, values, dest):
    jinja_env = jinja.Environment(loader=FileSystemLoader(srcdir))
    count = 0
    df = open(values['dag'], 'r')
    of = open("%s/dag.dag" % dest, "w")
    of.write("DOT %s/dag.dot UPDATE\n" % dest)
    in_parallel = False
    in_serial = False
    for line in df:
        if line == '<parallel>':
            in_parallel = True
            parallel_l = []
        elif line == '</parallel>':
            in_parallel = False
            parallels = " ".join(parallel_l)
            of.write("PARENT %s CHILD %s\n" % (last_serial, parallels))
            last_serial = parallels
        elif line == '<serial>':
            in_serial = True
        elif line == '</serial>':
            in_serial = False
        else:
            count = count + 1
            name = "stage_%d" % count
            res = parser.parse_args(line.strip().split(' ')[1:])
            thesevalues = values.copy()
            thesevalues.update(res)
            cf = open("%s/%s.cmd" % (dest, name), "w")
            csf = open("%s/%s.sh" % (dest, name), "w")
            cf.write(jinja_env.get_template('simple.cmd').render(**values))
            csf.write(jinja_env.get_template('simple.sh').render(**values))
            cf.close()
            of.write("JOB %s %s.cmd\n" % (name,name))
            if in_serial:
                of.write("PARENT %s CHILD %s\n" % (last_serial, name))
                last_serial = name
            if in_parallel:
                parallel_l.append(name)
    of.close()

def do_tarballs(values):
    # mooch from current code
    pass


def render_files(srcdir, values, dest):

    jinja_env = jinja.Environment(loader=FileSystemLoader(srcdir))
    flist = glob.glob('%s/*' % dir)

    # add destination dir to values for template
    values['cwd'] = dest

    for f in flist:
        bf = os.path.basename(f)
        of = open("%s/%s" % (dest, bf), "w")
        of.write(jinja_env.get_template(bf).render(**values))
        of.close()


def submit(f):
    f = glob.glob(f)[0]
    schedd, pool = get_schedd()
    os.system("condor_submit -name %s -pool %s %s" % (schedd, pool, f))


def submit_dag(f):
    f = glob.glob(f)[0]
    schedd, pool = get_schedd()
    os.system("condor_submit_dag %s" % (schedd, pool, f))

def get_principal():
    pass

def set_extras(args):
    args['user'] = os.environ['USER']
    args['ipaddr'] = socket.getwhatever()
    args['jobsub_version'] = 'lite_v1_0'
    args['kerberos_principal'] = get_principal()
    args['usage_model'] = 'ONSITE'

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--append_condor_requirements",
                        help="append condor requirements")
    parser.add_argument(
        "--blacklist", help="ensure that jobs do not land at these sites")
    parser.add_argument(
        "--cpu", help="request worker nodes have at least NUMBER cpus")
    parser.add_argument("--dag", help="submit and run a dagNabbit input file")
    parser.add_argument("--dataset_definition",
                        help="SAM dataset definition used in a Directed Acyclic Graph (DAG)")
    parser.add_argument("--debug", help="Turn on debugging")
    parser.add_argument(
        "--disk", help="Request worker nodes have at least NUMBER[UNITS] of disk space. If UNITS is not specified default is 'KB' (a typo in earlier versions said that default was 'MB', this was wrong). Allowed values for UNITS are 'KB','MB','GB', and 'TB'")
    parser.add_argument("-d", help="-d <tag> <dir> Writable directory $CONDOR_DIR_<tag> will exist on the execution node. After job completion, its contents will be moved to <dir> automatically Specify as many <tag>/<dir> pairs as you need.")
    parser.add_argument("--email-to", default="%s@fnal.gov" %
                        os.environ["USER"], help="email address to send job reports/summaries to (default is $USER@fnal.gov)")
    parser.add_argument("-e", "--environment", help=" -e ADDED_ENVIRONMENT exports this variable with its local value to worker node environment. For example export FOO='BAR'; jobsub -e FOO <more stuff> guarantees that the value of $FOO on the worker node is 'BAR' . Alternate format which does not require setting the env var first is the -e VAR=VAL, idiom which sets the value of $VAR to 'VAL' in the worker environment. The -e option can be used as many times in one jobsub_submit invocation as desired.")
    parser.add_argument("--expected-lifetime", help=" 'short'|'medium'|'long'|NUMBER[UNITS] Expected lifetime of the job. Used to match against resources advertising that they have REMAINING_LIFETIME seconds left. The shorter your EXPECTED_LIFTIME is, the more resources (aka slots, cpus) your job can potentially match against and the quicker it should start. If your job runs longer than EXPECTED_LIFETIME it *may* be killed by the batch system. If your specified EXPECTED_LIFETIME is too long your job may take a long time to match against a resource a sufficiently long REMAINING_LIFETIME. Valid inputs for this parameter are 'short', 'medium', IF [UNITS] is omitted, value is NUMBER seconds. Allowed values for UNITS are 's', 'm', 'h', 'd' representing seconds, minutes, etc.The values for 'short','medium',and 'long' are configurable by Grid Operations, they currently are '3h' , '8h' , and '85200s' but this may change in the future. Default value of EXPECTED_LIFETIME is currently '8h'.")
    parser.add_argument("-f", dest="input_file", default=[], action="append", help="INPUT_FILE at runtime, INPUT_FILE will be copied to directory $CONDOR_DIR_INPUT on the execution node. Example :-f /grid/data/minerva/my/input/file.xxx will be copied to $CONDOR_DIR_INPUT/file.xxx Specify as many -f INPUT_FILE_1 -f INPUT_FILE_2 args as you need. To copy file at submission time instead of run time, use -f dropbox://INPUT_FILE to copy the file.")
    parser.add_argument("--generate-email-summary", help="")
    parser.add_argument(
        "-G", "--group", help="Group/Experiment/Subgroup for priorities and accounting")
    parser.add_argument("-L", "--log_file",
                        help="Log file to hold log output from job.")
    parser.add_argument(
        "-l", "--lines", action="append", default=[""],help="Log file to hold log output from job.")
    parser.add_argument("-Q", "--mail_never", dest="mail", action="store_const",
                        const="never", default="never", help="never send mail about job results (default)")

    parser.add_argument("--mail_on_error", dest="mail", action="store_const",
                        const="on_error",  help="never send mail about job results (default)")
    parser.add_argument("--mail_always", dest="mail", action="store_const",
                        const="always",  help="never send mail about job results (default)")

    parser.add_argument("--maxConcurrent", help="max number of jobs running concurrently at given time.  Use in conjunction with -N option to protect a shared resource. Example: jobsub -N 1000 -maxConcurrent 20 will only run 20 jobs at a time until all 1000 have completed. This is implemented by running the jobs in a DAG. Normally when jobs are run with the -N option, they all have the same $CLUSTER number and differing, sequential $PROCESS numbers, and many submission scripts take advantage of this. When jobs are run with this option in a DAG each job has a different $CLUSTER number and a $PROCESS number of 0, which may break scripts that rely on the normal -N numbering scheme for $CLUSTER and $PROCESS. Groups of jobs run with this option will have the same $JOBSUBPARENTJOBID, each individual job will have a unique and sequential $JOBSUBJOBSECTION. Scripts may need modification to take this into account")
    parser.add_argument(
        "--memory", help="Request worker nodes have at least NUMBER[UNITS] of memory. If UNITS is not specified default is 'MB'.  Allowed values for UNITS are 'KB','MB','GB', and 'TB'")
    parser.add_argument("-N", help="submit N copies of this job. Each job will have access to the environment variable $PROCESS that provides the job number (0 to NUM-1), equivalent to the number following the decimal point in the job ID (the '2' in 134567.2).")
    parser.add_argument(
        "--OS", default=None, help="specify OS version of worker node. Example --OS=SL5 Comma seperated list '--OS=SL4,SL5,SL6' works as well . Default is any available OS")
    parser.add_argument("--overwrite_condor_requirements",
                        help="overwrite default condor requirements with supplied requirements")
    parser.add_argument("--resource-provides", action="append", default=[""], help="request specific resources by changing condor jdf file. For example: --resource-provides=CVMFS=OSG will add +DESIRED_CVMFS=\"OSG\" to the job classad attributes and '&&(CVMFS==\"OSG\")' to the job requirements")
    parser.add_argument(
        "--role", help="VOMS Role for priorities and accounting")
    parser.add_argument(
        "--site", help="submit jobs to these (comma-separated) sites")
    parser.add_argument("--subgroup", help=" Subgroup for priorities and accounting. See https://cdcvs.fnal.gov/redmine/projects/jobsub/wiki/ Jobsub_submit#Groups-Subgroups-Quotas-Priorities for more documentation on using --subgroup to set job quotas and priorities")
    parser.add_argument("--tar_file_name", default=[], action='append', help="                              d\ndropbox://PATH/TO/TAR_FILE\n tardir://PATH/TO/DIRECTORY \nspecify TAR_FILE or DIRECTORY to be transferred to worker node. TAR_FILE will be copied to an area specified in the jobsub server configuration, transferred to the job and unpacked there. TAR_FILE will be accessible to the user job on the worker node via the environment variable $INPUT_TAR_FILE. The unpacked contents will be in the same directory as $INPUT_TAR_FILE.")
    parser.add_argument(
        "--timeout", help="kill user job if still running after NUMBER[UNITS] of time . UNITS may be `s' for seconds (the default), `m' for minutes, `h' for hours or `d' h for days.")
    parser.add_argument("--use-cvmfs-dropbox",
                        help="use cvmfs for dropbox (default is pnfs)")
    parser.add_argument(
        "--verbose", help="dump internal state of program (useful for debugging)")

    parser.add_argument(
        "executable", help="executable to run, should be a file:///path/to/script url")
    parser.add_argument("arguments", default=[], nargs="+",
                        action="append", help="arguments to executable")
    args = parser.parse_args()
    tmp = get_tempdir()
    get_creds()
    do_tarballs(args)
    set_extras(args)

    if args.dag:
        d = '%s/templates/dag' % location
        parse_dagnabbit(parser, d, args.__dict__, tmp)
        submit_dag('%s/*.dag' % tmp)
    elif args.dataset_definition:
        d = '%s/templates/datset_dag' % location
        render_files(d, args.__dict__, tmp)
        submit_dag('%s/*.dag' % tmp)
    elif args.maxConcurrent:
        d = '%s/templates/maxconcurrent_dag' % location
        render_files(d, args.__dict__, tmp)
        submit_dag('%s/*.dag' % tmp)
    else:
        d = '%s/templates/simple' % location
        render_files(d, args.__dict__, tmp)
        submit('%s/*.sub' % tmp)
    cleanup(tmp)


if __name__ == '__main__':
    main()
