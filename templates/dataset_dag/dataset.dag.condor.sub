# Filename: {{submitdir}}/dataset.dag.condor.sub
# Generated by jobsub_lite based on condor_submit_dag {{submitdir}}/dataset.dag 
universe	= scheduler
executable	= {{submitdir}}/dagman_wrapper.sh
getenv		= True
output		= {{submitdir}}/dataset.dag.lib.out
error		= {{submitdir}}/dataset.dag.lib.err
log		= {{submitdir}}/dataset.dag.dagman.log
transfer_output_files = dataset.dag.nodes.log
remove_kill_sig	= SIGUSR1
+OtherJobRemoveRequirements	= "DAGManJobId =?= $(cluster)"
# Note: default on_exit_remove expression:
# ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2))
# attempts to ensure that DAGMan is automatically
# requeued by the schedd if it exits abnormally or
# is killed (e.g., during a reboot).
on_exit_remove	= (ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2))
copy_to_spool	= False
arguments	= "-p 0 -f -l . -Lockfile dataset.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag {{submitdir}}/dataset.dag -Suppress_notification -CsdVersion $CondorVersion:' '8.8.10' 'Aug' '12' '2020' 'PackageID:' '8.8.10-1.3' '$ -Dagman /usr/bin/condor_dagman"
environment	= _CONDOR_SCHEDD_ADDRESS_FILE=/var/lib/condor/spool/.schedd_address;_CONDOR_MAX_DAGMAN_LOG=0;_CONDOR_SCHEDD_DAEMON_AD_FILE=/var/lib/condor/spool/.schedd_classad;_CONDOR_DAGMAN_LOG={{submitdir}}/dataset.dag.dagman.out

+JobsubClientDN="{{clientdn}}"
+JobsubClientIpAddress="{{ipaddr}}"
+Owner="{{user}}"
+JobsubServerVersion="{{jobsub_version}}"
+JobsubClientVersion="{{jobsub_version}}"
+JobsubClientKerberosPrincipal="{{kerberosprincipal}}"
x509userproxy = {{proxy}}
{{lines|join("\n+")}}

{% if subgroup %}
+AccountingGroup = "group_{{group}}.{{subgroup}}.{{user}}"
{% else %}
+AccountingGroup = "group_{{group}}.{{user}}"
{% endif %}

+Jobsub_Group="{{group}}"
+JobsubJobId="$(CLUSTER).$(PROCESS)@{{schedd}}"

queue
