#!/usr/bin/env python

import os
import os.path
import sys
import glob
import argparse
import jinja2 as jinja
import ifdh
import requests
import random
import socket
import subprocess
import datetime
import uuid

random.seed()

def parse_dagnabbit(parser, srcdir, values, dest):
    ''' 
         parse a dagnabbit dag file generating a .dag file and .cmd files 
         in the dest directory, using global cmdline options from values
         along with ones parsed from dagnabbit file
    '''
    jinja_env = jinja.Environment(loader=jinja.FileSystemLoader(srcdir))
    count = 0
    df = open(values['dag'], 'r')
    of = open("%s/dag.dag" % dest, "w")
    of.write("DOT %s/dag.dot UPDATE\n" % dest)
    in_parallel = False
    in_serial = False
    for line in df:
        if line == '<parallel>':
            in_parallel = True
            parallel_l = []
        elif line == '</parallel>':
            in_parallel = False
            parallels = " ".join(parallel_l)
            of.write("PARENT %s CHILD %s\n" % (last_serial, parallels))
            last_serial = parallels
        elif line == '<serial>':
            in_serial = True
        elif line == '</serial>':
            in_serial = False
        else:
            count = count + 1
            name = "stage_%d" % count
            res = parser.parse_args(line.strip().split(' ')[1:])
            thesevalues = values.copy()
            thesevalues.update(res)
            cf = open("%s/%s.cmd" % (dest, name), "w")
            csf = open("%s/%s.sh" % (dest, name), "w")
            cf.write(jinja_env.get_template('simple.cmd').render(**thesevalues))
            csf.write(jinja_env.get_template('simple.sh').render(**thesevalues))
            cf.close()
            of.write("JOB %s %s.cmd\n" % (name,name))
            if in_serial:
                of.write("PARENT %s CHILD %s\n" % (last_serial, name))
                last_serial = name
            if in_parallel:
                parallel_l.append(name)
    of.close()

def tar_up(directory, excludes):
    ''' build path/to/directory.tar from path/to/directory '''
    tarfile = "%s.tar.gz" % directory
    if not excludes:
        excludes = os.path.dirname(__FILE__) + '/../etc/default_excludes'
    excludes = "--exclude-from %s" % excludes
    os.system("tar czvf %s %s %s" % (tarfile, excludes, directory))
    return tarfile

def slurp_file(fname):
     ''' pull in a tarfile while computing its hash '''
     h = hashlib.sha256()
     tfl=[]
     with open(fname,"r") as f:
         tff = f.read(4096)
         h.update(tff)
         tfl.append(tff)
         while tff:
             tff = f.read(4096)
             h.update(tff)
             tfl.append(tff)
     return h.hexdigest(), ''.join(tfl)

ih = None
def get_creds():
     global ih
     if not ih:
         ih = ifdh.ifdh()
     return ih.getProxy()

def do_tarballs(args):
    ''' handle tarfile argument;  we could have: 
           a directory with tardir: prefix to tar up and upload
           a tarfile with dropbox: prefix to upload
           a plain path to just use
        we convert the argument to the next type as we go...
    '''
   
    res = []
    for tfn in args.tar_file_name:
        if tfn.startswith("tardir:"):
             # tar it up, pretend they gave us dropbox:
             tarfile = tar_up(args.tar_file_name[7:], args.tarball_exclusion_file)
             tfn = "dropbox:%s" % tarfile

        if tfn.startswith("dropbox:"):
             # move it to dropbox area, pretend they gave us plain path
             
             digest,tf = slurp_file(args.tar_file_name[8])
             proxy = get_creds()

             cid = ''.join( (args.group, '%2F', digest) )
             location = pubapi_update(cid, proxy)
             if not location:
                 location = pubapi_publish(cid, tf, proxy)

             tfn = location
        res.append(tfn)
    args.tar_file_name = res 

DROPBOX_SERVERS=("rcds01.fnal.gov", "rcds02.fnal.gov")

def pubapi_update(cid, proxy):
    ''' make pubapi update call to check if we already have this tarfile,
        return path.
    '''
    dropbox_server = random.choice(DROPBOX_SERVERS)
    url = "https://%s/pubapi/update?cid=%s" % (dropbox_server, cid)
    res = requests.GET(url, args, cert=(proxy,proxy))
    if res.text()[:8] == 'PRESENT:':
        return res.text()[8:]
    else:
        return None

def pubapi_publish(cid, tf, proxy):
    ''' make pubapi publish call to upload this tarfile, return path'''
    dropbox_server = random.choice(DROPBOX_SERVERS)
    url = "https://%s/pubapi/publish?cid=%s" % (dropbox_server, cid)
    res = requests.POST(url, cert=(proxy,proxy), data=tf)
    if res.text()[:8] == 'PRESENT:':
        return res.text()[8:]
    else:
        return None

def render_files(srcdir, values, dest):
    ''' use jinja to render the templates from srcdir into the dest directory
        using values dict for substitutions
    '''
    print("trying to render files from %s\n" % srcdir)
    jinja_env = jinja.Environment(loader=jinja.FileSystemLoader(srcdir))
    flist = glob.glob('%s/*' % srcdir)

    # add destination dir to values for template
    values['cwd'] = dest

    for f in flist:
        print("rendering: %s" % f)
        bf = os.path.basename(f)
        of = open("%s/%s" % (dest, bf), "w")
        of.write(jinja_env.get_template(bf).render(**values))
        of.close()

SCHEDDS = (("jobsub01.fnal.gov","default"),("jobsub02.fnal.gov","default"))
def get_schedd():
    ''' FIXME: this picks a schedd and pool. it should be smarter somehow '''
    return random.choice(SCHEDDS)

def submit(f):
    ''' Actually submit the job '''
    print("submitting: %s" % f)
    fl = glob.glob(f)
    if fl:
        f = fl[0]
    schedd, pool = get_schedd()
    os.system("echo condor_submit -name %s -pool %s %s" % (schedd, pool, f))

def submit_dag(f):
    ''' Actually submit the dag '''
    fl = glob.glob(f)
    if fl:
        f = fl[0]
    schedd, pool = get_schedd()
    os.system("echo condor_submit_dag %s" % (schedd, pool, f))

def cleanup(tmp):
    print("remember to clean up %s" % tmp)
    #os.system("rm -rf %s" % tmp)

def get_principal():
    princ = None
    p = subprocess.Popen(["/usr/bin/klist"], stdout=subprocess.PIPE)
    line = p.stdout.readline()
    line = p.stdout.readline()
    princ = line[line.find(":")+2:-1]
    p.stdout.close()
    p.wait()
    return princ

def set_extras(args):
    args.user = os.environ['USER']
    ai = socket.getaddrinfo(socket.gethostname(),80)
    if ai:
        args.ipaddr = ai[-1][-1][0]
    else:
        args.ipaddr = 'unknown'
    args.jobsub_version = 'lite_v1_0'
    args.kerberos_principal = get_principal()
    args.usage_model = 'ONSITE'
    args.uuid = str(uuid.uuid4())
    args.date = datetime.datetime.now().strftime("%Y_%m_%d_%H%M%S")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--append_condor_requirements",
                        help="append condor requirements")
    parser.add_argument(
        "--blacklist", help="enusure that jobs do not land at these sites")
    parser.add_argument(
        "--cpu", help="request worker nodes have at least NUMBER cpus")
    parser.add_argument("--dag", help="submit and run a dagNabbit input file")
    parser.add_argument("--dataset_definition",
                        help="SAM dataset definition used in a Directed Acyclic Graph (DAG)")
    parser.add_argument("--debug", help="Turn on debugging")
    parser.add_argument(
        "--disk", help="Request worker nodes have at least NUMBER[UNITS] of disk space. If UNITS is not specified default is 'KB' (a typo in earlier versions said that default was 'MB', this was wrong). Allowed values for UNITS are 'KB','MB','GB', and 'TB'")
    parser.add_argument("-d", help="-d <tag> <dir> Writable directory $CONDOR_DIR_<tag> will exist on the execution node. After job completion, its contents will be moved to <dir> automatically Specify as many <tag>/<dir> pairs as you need.")
    parser.add_argument("--email-to", default="%s@fnal.gov" %
                        os.environ["USER"], help="email address to send job reports/summaries to (default is $USER@fnal.gov)")
    parser.add_argument("-e", "--environment", help=" -e ADDED_ENVIRONMENT exports this variable with its local value to worker node environment. For example export FOO='BAR'; jobsub -e FOO <more stuff> guarantees that the value of $FOO on the worker node is 'BAR' . Alternate format which does not require setting the env var first is the -e VAR=VAL, idiom which sets the value of $VAR to 'VAL' in the worker environment. The -e option can be used as many times in one jobsub_submit invocation as desired.")
    parser.add_argument("--expected-lifetime", help=" 'short'|'medium'|'long'|NUMBER[UNITS] Expected lifetime of the job. Used to match against resources advertising that they have REMAINING_LIFETIME seconds left. The shorter your EXPECTED_LIFTIME is, the more resources (aka slots, cpus) your job can potentially match against and the quicker it should start. If your job runs longer than EXPECTED_LIFETIME it *may* be killed by the batch system. If your specified EXPECTED_LIFETIME is too long your job may take a long time to match against a resource a sufficiently long REMAINING_LIFETIME. Valid inputs for this parameter are 'short', 'medium', IF [UNITS] is omitted, value is NUMBER seconds. Allowed values for UNITS are 's', 'm', 'h', 'd' representing seconds, minutes, etc.The values for 'short','medium',and 'long' are configurable by Grid Operations, they currently are '3h' , '8h' , and '85200s' but this may change in the future. Default value of EXPECTED_LIFETIME is currently '8h'.")
    parser.add_argument("-f", dest="input_file", default=[], action="append", help="INPUT_FILE at runtime, INPUT_FILE will be copied to directory $CONDOR_DIR_INPUT on the execution node. Example :-f /grid/data/minerva/my/input/file.xxx will be copied to $CONDOR_DIR_INPUT/file.xxx Specify as many -f INPUT_FILE_1 -f INPUT_FILE_2 args as you need. To copy file at submission time instead of run time, use -f dropbox://INPUT_FILE to copy the file.")
    parser.add_argument("--generate-email-summary", help="")
    parser.add_argument(
        "-G", "--group", help="Group/Experiment/Subgroup for priorities and accounting")
    parser.add_argument("-L", "--log_file",
                        help="Log file to hold log output from job.")
    parser.add_argument(
        "-l", "--lines", action="append", default=[""],help="Log file to hold log output from job.")
    parser.add_argument("-Q", "--mail_never", dest="mail", action="store_const",
                        const="never", default="never", help="never send mail about job results (default)")

    parser.add_argument("--mail_on_error", dest="mail", action="store_const",
                        const="on_error",  help="never send mail about job results (default)")
    parser.add_argument("--mail_always", dest="mail", action="store_const",
                        const="always",  help="never send mail about job results (default)")

    parser.add_argument("--maxConcurrent", help="max number of jobs running concurrently at given time.  Use in conjunction with -N option to protect a shared resource. Example: jobsub -N 1000 -maxConcurrent 20 will only run 20 jobs at a time until all 1000 have completed. This is implemented by running the jobs in a DAG. Normally when jobs are run with the -N option, they all have the same $CLUSTER number and differing, sequential $PROCESS numbers, and many submission scripts take advantage of this. When jobs are run with this option in a DAG each job has a different $CLUSTER number and a $PROCESS number of 0, which may break scripts that rely on the normal -N numbering scheme for $CLUSTER and $PROCESS. Groups of jobs run with this option will have the same $JOBSUBPARENTJOBID, each individual job will have a unique and sequential $JOBSUBJOBSECTION. Scripts may need modification to take this into account")
    parser.add_argument(
        "--memory", help="Request worker nodes have at least NUMBER[UNITS] of memory. If UNITS is not specified default is 'MB'.  Allowed values for UNITS are 'KB','MB','GB', and 'TB'")
    parser.add_argument("-N", default=1, help="submit N copies of this job. Each job will have access to the environment variable $PROCESS that provides the job number (0 to NUM-1), equivalent to the number following the decimal point in the job ID (the '2' in 134567.2).")
    parser.add_argument(
        "--OS", default=None, help="specify OS version of worker node. Example --OS=SL5 Comma seperated list '--OS=SL4,SL5,SL6' works as well . Default is any available OS")
    parser.add_argument("--overwrite_condor_requirements",
                        help="overwrite default condor requirements with supplied requirements")
    parser.add_argument("--resource-provides", action="append", default=[""], help="request specific resources by changing condor jdf file. For example: --resource-provides=CVMFS=OSG will add +DESIRED_CVMFS=\"OSG\" to the job classad attributes and '&&(CVMFS==\"OSG\")' to the job requirements")
    parser.add_argument(
        "--role", help="VOMS Role for priorities and accounting")
    parser.add_argument(
        "--site", help="submit jobs to these (comma-separated) sites")
    parser.add_argument("--subgroup", help=" Subgroup for priorities and accounting. See https://cdcvs.fnal.gov/redmine/projects/jobsub/wiki/ Jobsub_submit#Groups-Subgroups-Quotas-Priorities for more documentation on using --subgroup to set job quotas and priorities")
    parser.add_argument("--tar_file_name", default=[], action='append', help="                              d\ndropbox://PATH/TO/TAR_FILE\n tardir://PATH/TO/DIRECTORY \nspecify TAR_FILE or DIRECTORY to be transferred to worker node. TAR_FILE will be copied to an area specified in the jobsub server configuration, transferred to the job and unpacked there. TAR_FILE will be accessible to the user job on the worker node via the environment variable $INPUT_TAR_FILE. The unpacked contents will be in the same directory as $INPUT_TAR_FILE.")
    parser.add_argument("--tarball-exclusion-file", default=None, help="File with patterns to exclude from tarffile creation")
    parser.add_argument(
        "--timeout", help="kill user job if still running after NUMBER[UNITS] of time . UNITS may be `s' for seconds (the default), `m' for minutes, `h' for hours or `d' h for days.")
    parser.add_argument("--use-cvmfs-dropbox",
                        help="use cvmfs for dropbox (default is pnfs)")
    parser.add_argument(
        "--verbose", help="dump internal state of program (useful for debugging)")

    parser.add_argument(
        "executable", help="executable to run, should be a file:///path/to/script url")
    parser.add_argument("arguments", default=[], nargs="*",
                        action="append", help="arguments to executable")
    args = parser.parse_args()
    tmp = "%s/js%d" % (os.environ.get('TMPDIR','/tmp'), os.getpid())
    if not os.path.isdir(tmp):
        os.makedirs(tmp)
    get_creds()
    do_tarballs(args)
    set_extras(args)
    
    # we are in prefix/bin/jobsub_submit...
    # so prefix is:
    prefix = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    if args.dag:
        d = '%s/templates/dag' % prefix
        parse_dagnabbit(parser, d, args.__dict__, tmp)
        submit_dag('%s/*.dag' % tmp)
    elif args.dataset_definition:
        d = '%s/templates/datset_dag' % prefix
        render_files(d, args.__dict__, tmp)
        submit_dag('%s/*.dag' % tmp)
    elif args.maxConcurrent:
        d = '%s/templates/maxconcurrent_dag' % prefix
        render_files(d, args.__dict__, tmp)
        submit_dag('%s/*.dag' % tmp)
    else:
        d = '%s/templates/simple' % prefix
        render_files(d, args.__dict__, tmp)
        submit('%s/*.cmd' % tmp)
    cleanup(tmp)


if __name__ == '__main__':
    main()
